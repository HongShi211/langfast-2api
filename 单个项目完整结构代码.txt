é¡¹ç›® 'langfast-2api' çš„ç»“æ„æ ‘:
ğŸ“‚ langfast-2api/
    ğŸ“„ .env
    ğŸ“„ .env.example
    ğŸ“„ Dockerfile
    ğŸ“„ docker-compose.yml
    ğŸ“„ main.py
    ğŸ“„ nginx.conf
    ğŸ“„ requirements.txt
    ğŸ“‚ app/
        ğŸ“‚ core/
            ğŸ“„ __init__.py
            ğŸ“„ config.py
        ğŸ“‚ providers/
            ğŸ“„ __init__.py
            ğŸ“„ base_provider.py
            ğŸ“„ langfast_provider.py
        ğŸ“‚ services/
            ğŸ“„ credential_manager.py
            ğŸ“„ socketio_manager.py
        ğŸ“‚ utils/
            ğŸ“„ sse_utils.py
================================================================================

--- æ–‡ä»¶è·¯å¾„: .env ---

# ====================================================================
# langfast-2api é…ç½®æ–‡ä»¶æ¨¡æ¿
# ====================================================================
#
# è¯·å°†æ­¤æ–‡ä»¶é‡å‘½åä¸º ".env" å¹¶æŒ‰éœ€ä¿®æ”¹ã€‚
#

# --- æ ¸å¿ƒå®‰å…¨é…ç½® (å¿…é¡»è®¾ç½®) ---
# ç”¨äºä¿æŠ¤æ‚¨ API æœåŠ¡çš„è®¿é—®å¯†é’¥ã€‚
API_MASTER_KEY=1

# --- éƒ¨ç½²é…ç½® (å¯é€‰) ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8088

# --- å‡­è¯æ± é…ç½® (å¯é€‰) ---
# å‡­è¯æ± ä¸­ç»´æŠ¤çš„æœ€å°åŒ¿åè´¦å·æ•°é‡
MIN_CREDENTIALS=5
# å‡­è¯æ± ä¸­ç»´æŠ¤çš„æœ€å¤§åŒ¿åè´¦å·æ•°é‡
MAX_CREDENTIALS=10



--- æ–‡ä»¶è·¯å¾„: .env.example ---

# ====================================================================
# langfast-2api é…ç½®æ–‡ä»¶æ¨¡æ¿
# ====================================================================
#
# è¯·å°†æ­¤æ–‡ä»¶é‡å‘½åä¸º ".env" å¹¶æŒ‰éœ€ä¿®æ”¹ã€‚
#

# --- æ ¸å¿ƒå®‰å…¨é…ç½® (å¿…é¡»è®¾ç½®) ---
# ç”¨äºä¿æŠ¤æ‚¨ API æœåŠ¡çš„è®¿é—®å¯†é’¥ã€‚
API_MASTER_KEY=sk-langfast-2api-default-key-please-change-me

# --- éƒ¨ç½²é…ç½® (å¯é€‰) ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8088

# --- å‡­è¯æ± é…ç½® (å¯é€‰) ---
# å‡­è¯æ± ä¸­ç»´æŠ¤çš„æœ€å°åŒ¿åè´¦å·æ•°é‡
MIN_CREDENTIALS=5
# å‡­è¯æ± ä¸­ç»´æŠ¤çš„æœ€å¤§åŒ¿åè´¦å·æ•°é‡
MAX_CREDENTIALS=10



--- æ–‡ä»¶è·¯å¾„: Dockerfile ---

# /Dockerfile
FROM python:3.10-slim

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

COPY . .

RUN useradd --create-home appuser && \
    chown -R appuser:appuser /app
USER appuser

EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]


--- æ–‡ä»¶è·¯å¾„: docker-compose.yml ---

# /docker-compose.yml
services:
  nginx:
    image: nginx:latest
    container_name: langfast-2api-nginx
    restart: always
    ports:
      - "${NGINX_PORT:-8088}:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - app
    networks:
      - langfast-net

  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: langfast-2api-app
    restart: unless-stopped
    env_file:
      - .env
    networks:
      - langfast-net

networks:
  langfast-net:
    driver: bridge


--- æ–‡ä»¶è·¯å¾„: main.py ---

import sys
from contextlib import asynccontextmanager
from typing import Optional

from fastapi import FastAPI, Request, HTTPException, Depends, Header
from fastapi.responses import JSONResponse, StreamingResponse
from loguru import logger

from app.core.config import settings
from app.providers.langfast_provider import LangfastProvider

# --- é…ç½® Loguru ---
logger.remove()
logger.add(
    sys.stdout,
    level="INFO",
    format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
           "<level>{level: <8}</level> | "
           "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    colorize=True
)

provider: Optional[LangfastProvider] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global provider
    logger.info(f"åº”ç”¨å¯åŠ¨ä¸­... {settings.APP_NAME} v{settings.APP_VERSION}")
    provider = LangfastProvider()
    await provider.initialize()
    logger.info(f"æœåŠ¡å°†åœ¨ http://localhost:{settings.NGINX_PORT} ä¸Šå¯ç”¨")
    yield
    await provider.close()
    logger.info("åº”ç”¨å…³é—­ã€‚")

app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description=settings.DESCRIPTION,
    lifespan=lifespan
)

async def verify_api_key(authorization: Optional[str] = Header(None)):
    if settings.API_MASTER_KEY and settings.API_MASTER_KEY != "1":
        if not authorization or "bearer" not in authorization.lower():
            raise HTTPException(status_code=401, detail="éœ€è¦ Bearer Token è®¤è¯ã€‚")
        token = authorization.split(" ")[-1]
        if token != settings.API_MASTER_KEY:
            raise HTTPException(status_code=403, detail="æ— æ•ˆçš„ API Keyã€‚")

@app.post("/v1/chat/completions", dependencies=[Depends(verify_api_key)])
async def chat_completions(request: Request):
    try:
        request_data = await request.json()
        return await provider.chat_completion(request_data)
    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(e)}")

@app.get("/v1/models", dependencies=[Depends(verify_api_key)])
async def list_models():
    return await provider.get_models()

@app.get("/", summary="æ ¹è·¯å¾„", include_in_schema=False)
def root():
    return {"message": f"æ¬¢è¿æ¥åˆ° {settings.APP_NAME} v{settings.APP_VERSION}. æœåŠ¡è¿è¡Œæ­£å¸¸ã€‚"}


--- æ–‡ä»¶è·¯å¾„: nginx.conf ---

# /nginx.conf
worker_processes auto;

events {
    worker_connections 1024;
}

http {
    upstream langfast_backend {
        # ip_hash ç¡®ä¿æ¥è‡ªåŒä¸€å®¢æˆ·ç«¯çš„è¯·æ±‚è¢«è½¬å‘åˆ°åŒä¸€ä¸ª worker,
        # è¿™å¯¹äºä¿æŒä¼šè¯ä¸Šä¸‹æ–‡çš„è¿ç»­æ€§è‡³å…³é‡è¦ã€‚
        ip_hash;
        server app:8000;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://langfast_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # æµå¼ä¼ è¾“ä¼˜åŒ–
            proxy_buffering off;
            proxy_cache off;
            proxy_set_header Connection '';
            proxy_http_version 1.1;
            chunked_transfer_encoding off;
        }
    }
}


--- æ–‡ä»¶è·¯å¾„: requirements.txt ---

fastapi
uvicorn[standard]
pydantic-settings
python-dotenv
loguru
httpx
python-socketio[asyncio_client]


--- æ–‡ä»¶è·¯å¾„: app\core\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\core\config.py ---

import os
from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import List, Optional, Dict

class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding='utf-8',
        extra="ignore"
    )

    APP_NAME: str = "langfast-2api"
    APP_VERSION: str = "1.0.0"
    DESCRIPTION: str = "ä¸€ä¸ªå°† langfa.st è½¬æ¢ä¸ºå…¼å®¹ OpenAI æ ¼å¼ API çš„é«˜æ€§èƒ½ä»£ç†ã€‚"

    # --- å®‰å…¨ä¸éƒ¨ç½² ---
    API_MASTER_KEY: Optional[str] = "1"
    NGINX_PORT: int = 8088

    # --- ä¸Šæ¸¸ API ---
    SUPABASE_URL: str = "https://yzaaxwkjukajpwqflndu.supabase.co"
    SUPABASE_ANON_KEY: str = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Inl6YWF4d2tqdWthanB3cWZsbmR1Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDMxNTgzNzUsImV4cCI6MjA1ODczNDM3NX0.tDu7BsFUOV4aPo8U-dsokQJSRw29LQWT8JfDqQe5MoI"
    SOCKET_URL: str = "wss://langfast-prompt-runner-35808038077.us-central1.run.app"
    
    # --- å‡­è¯æ± é…ç½® ---
    MIN_CREDENTIALS: int = 5
    MAX_CREDENTIALS: int = 10

    # --- æ¨¡å‹åˆ—è¡¨ ---
    KNOWN_MODELS: List[str] = [
        "gpt-5", "gpt-5-mini", "gpt-5-nano",
        "gpt-4.5-preview", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano",
        "gpt-4o", "gpt-4o-mini", "o1", "o1-mini", "o3", "o3-mini", "o4-mini",
        "gpt-4-turbo", "gpt-4", "gpt-3.5-turbo"
    ]
    DEFAULT_MODEL: str = "gpt-4o-mini"

settings = Settings()


--- æ–‡ä»¶è·¯å¾„: app\providers\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\providers\base_provider.py ---

from abc import ABC, abstractmethod
from typing import Dict, Any
from fastapi.responses import StreamingResponse, JSONResponse

class BaseProvider(ABC):
    @abstractmethod
    async def chat_completion(
        self,
        request_data: Dict[str, Any]
    ) -> StreamingResponse:
        pass

    @abstractmethod
    async def get_models(self) -> JSONResponse:
        pass


--- æ–‡ä»¶è·¯å¾„: app\providers\langfast_provider.py ---

import httpx
import json
import uuid
import time
import asyncio
import base64
from typing import Dict, Any, AsyncGenerator, Optional

from fastapi.responses import StreamingResponse, JSONResponse
from loguru import logger

from app.core.config import settings
from app.services.credential_manager import CredentialManager
from app.services.socketio_manager import SocketIOManager
from app.utils.sse_utils import create_sse_data, create_chat_completion_chunk, DONE_CHUNK, create_chat_completion_response

# æ—¥å¿—è®°å½•è¾…åŠ©å‡½æ•° (æ— å˜åŠ¨)
async def log_request(request):
    logger.debug(f"--- [HTTP Request] --->")
    logger.debug(f"Request: {request.method} {request.url}")
    logger.debug(f"Headers: {request.headers}")
    if request.content:
        try:
            content = json.loads(request.content)
            logger.debug(f"Body: \n{json.dumps(content, indent=2, ensure_ascii=False)}")
        except (json.JSONDecodeError, UnicodeDecodeError):
            logger.debug(f"Body (raw): {request.content}")
    logger.debug(f"--- [HTTP Request] ---")

async def log_response(response):
    await response.aread()
    logger.debug(f"<--- [HTTP Response] ---")
    logger.debug(f"Status: {response.status_code}")
    logger.debug(f"Headers: {response.headers}")
    try:
        content = response.json()
        logger.debug(f"Body: \n{json.dumps(content, indent=2, ensure_ascii=False)}")
    except (json.JSONDecodeError, UnicodeDecodeError):
        logger.debug(f"Body (raw): {response.text}")
    logger.debug(f"<--- [HTTP Response] ---")

# JWT è§£ç è¾…åŠ©å‡½æ•° (æ— å˜åŠ¨)
def get_user_id_from_token(token: str) -> Optional[str]:
    if not token: return None
    try:
        parts = token.split('.')
        if len(parts) != 3: return None
        payload_b64 = parts[1]
        payload_b64 += '=' * (-len(payload_b64) % 4)
        payload_json = base64.b64decode(payload_b64).decode('utf-8')
        payload = json.loads(payload_json)
        return payload.get('sub')
    except Exception as e:
        logger.error(f"ä» token è§£ç ç”¨æˆ· ID æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return None

class LangfastProvider:
    def __init__(self):
        self.credential_manager = CredentialManager()
        self.initiate_url = f"{settings.SUPABASE_URL}/functions/v1/initiate-prompt-run"
        self.client = httpx.AsyncClient(
            timeout=30.0,
            event_hooks={'request': [log_request], 'response': [log_response]}
        )

    async def initialize(self):
        await self.credential_manager.initialize()

    async def close(self):
        await self.credential_manager.close()
        await self.client.aclose()

    # --- [ä¿®æ”¹] é‡æ„ chat_completion ä»¥æ”¯æŒæµå¼å’Œéæµå¼ ---
    async def chat_completion(self, request_data: Dict[str, Any]) -> StreamingResponse | JSONResponse:
        is_streaming = request_data.get("stream", False)
        request_id = f"chatcmpl-{uuid.uuid4()}"
        model = request_data.get("model", settings.DEFAULT_MODEL)

        try:
            access_token = await self.credential_manager.get_credential()
            user_id = get_user_id_from_token(access_token)
            if not user_id:
                raise Exception("æ— æ³•ä»å‡­è¯ä¸­è§£æç”¨æˆ·IDï¼Œè¯·æ±‚ä¸­æ–­ã€‚")

            socket_manager = SocketIOManager(access_token, settings.SOCKET_URL)
            await socket_manager.connect()

            headers = {
                "apikey": settings.SUPABASE_ANON_KEY,
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/json"
            }
            payload = self._prepare_payload(request_data, user_id)

            logger.info(f"æ­£åœ¨å‘ä¸Šæ¸¸å‘é€èŠå¤©è§¦å‘è¯·æ±‚ (stream={is_streaming})...")
            response = await self.client.post(self.initiate_url, headers=headers, json=payload)
            response.raise_for_status()
            logger.success("èŠå¤©è§¦å‘è¯·æ±‚æˆåŠŸã€‚ç­‰å¾… Socket.IO æ•°æ®...")

            if is_streaming:
                return StreamingResponse(
                    self._stream_generator(request_id, model, socket_manager),
                    media_type="text/event-stream"
                )
            else:
                final_data = await self._collect_full_response(socket_manager)
                await socket_manager.disconnect()
                response_json = create_chat_completion_response(
                    request_id,
                    model,
                    final_data.get("content", ""),
                    final_data.get("finish_reason", "stop")
                )
                return JSONResponse(content=response_json)

        except Exception as e:
            logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}", exc_info=True)
            if is_streaming:
                async def error_stream():
                    chunk = create_chat_completion_chunk(request_id, model, f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {e}", "stop")
                    yield create_sse_data(chunk)
                    yield DONE_CHUNK
                return StreamingResponse(error_stream(), media_type="text/event-stream")
            else:
                return JSONResponse(
                    status_code=500,
                    content={"error": {"message": f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {e}", "type": "server_error"}}
                )

    # --- [ä¿®æ”¹] å®ç°å¢é‡è®¡ç®—çš„æµå¼ç”Ÿæˆå™¨ ---
    async def _stream_generator(self, request_id: str, model: str, socket_manager: SocketIOManager) -> AsyncGenerator[str, None]:
        last_content = ""
        try:
            while not socket_manager.is_finished.is_set():
                data = await socket_manager.get_data()
                if data is None:
                    break

                full_content = data.get("content", "")
                if full_content and full_content != last_content:
                    # è®¡ç®—å¢é‡
                    delta_content = full_content[len(last_content):]
                    chunk = create_chat_completion_chunk(request_id, model, delta_content)
                    yield create_sse_data(chunk)
                    last_content = full_content
        
        except Exception as e:
            logger.error(f"å¤„ç†æµæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
            error_chunk = create_chat_completion_chunk(request_id, model, f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {e}", "stop")
            yield create_sse_data(error_chunk)
        finally:
            final_chunk = create_chat_completion_chunk(request_id, model, "", "stop")
            yield create_sse_data(final_chunk)
            yield DONE_CHUNK
            await socket_manager.disconnect()

    # --- [æ–°å¢] ç”¨äºéæµå¼å“åº”çš„å‡½æ•° ---
    async def _collect_full_response(self, socket_manager: SocketIOManager) -> Dict[str, Any]:
        final_data = {}
        while not socket_manager.is_finished.is_set():
            data = await socket_manager.get_data()
            if data is None:
                break
            final_data = data # ä¸æ–­è¦†ç›–ï¼Œç›´åˆ°æœ€åä¸€ä¸ª
        return final_data

    def _prepare_payload(self, request_data: Dict[str, Any], user_id: str) -> Dict[str, Any]:
        prompt_meta = {
            "model": request_data.get("model", settings.DEFAULT_MODEL),
            "messages": request_data.get("messages", []),
            "temperature": request_data.get("temperature", 0.7),
            "top_p": request_data.get("top_p", 1),
            "frequency_penalty": request_data.get("frequency_penalty", 0),
            "presence_penalty": request_data.get("presence_penalty", 0),
            "max_completion_tokens": request_data.get("max_tokens", 16384),
            "response_format": "text",
            "stream": True, # ä¸Šæ¸¸å§‹ç»ˆä½¿ç”¨æµå¼ï¼Œç”±æˆ‘ä»¬å†³å®šå¦‚ä½•è¿”å›ç»™å®¢æˆ·ç«¯
        }
        return {
            "run_id": str(uuid.uuid4()),
            "prompt_id": "6456df9f-f4fd-42b3-97ff-df29af7188b7",
            "prompt_meta": prompt_meta,
            "test_cases": [],
            "created_by": user_id
        }

    async def get_models(self) -> JSONResponse:
        return JSONResponse(content={
            "object": "list",
            "data": [{"id": name, "object": "model", "created": int(time.time()), "owned_by": "lzA6"} for name in settings.KNOWN_MODELS]
        })


--- æ–‡ä»¶è·¯å¾„: app\services\credential_manager.py ---

import asyncio
import httpx
from collections import deque
from typing import Optional
from loguru import logger
from app.core.config import settings

class CredentialManager:
    def __init__(self):
        self.credentials = deque()
        self.lock = asyncio.Lock()
        self.signup_url = f"{settings.SUPABASE_URL}/auth/v1/signup"
        self.headers = {
            "apikey": settings.SUPABASE_ANON_KEY,
            "Content-Type": "application/json"
        }
        # ä¿®æ­£ï¼šä¸º httpx å®¢æˆ·ç«¯è®¾ç½®è¶…æ—¶
        self.client = httpx.AsyncClient(timeout=30.0)

    async def initialize(self):
        logger.info("æ­£åœ¨åˆå§‹åŒ–å‡­è¯ç®¡ç†å™¨...")
        await self.maintain_credentials()

    async def _create_new_credential(self) -> Optional[str]:
        try:
            logger.info("æ­£åœ¨æ³¨å†Œæ–°çš„åŒ¿åè´¦å·...")
            # ä¿®æ­£ï¼šåŒ¿åæ³¨å†Œçš„ body æ˜¯ä¸€ä¸ªç©º json
            response = await self.client.post(self.signup_url, headers=self.headers, json={})
            response.raise_for_status()
            data = response.json()
            access_token = data.get("access_token")
            if access_token:
                logger.success("æˆåŠŸè·å–æ–°çš„åŒ¿åå‡­è¯ã€‚")
                return access_token
            else:
                logger.error(f"æ³¨å†ŒåŒ¿åè´¦å·å¤±è´¥ï¼Œå“åº”ä¸­ç¼ºå°‘ 'access_token': {data}")
                return None
        except httpx.RequestError as e:
            # ä¿®æ­£ï¼šæ•è·å¹¶è®°å½•æ›´å…·ä½“çš„ç½‘ç»œé”™è¯¯
            logger.error(f"æ³¨å†ŒåŒ¿åè´¦å·æ—¶å‘ç”Ÿç½‘ç»œè¯·æ±‚é”™è¯¯: {type(e).__name__} - {e}")
            return None
        except Exception as e:
            logger.error(f"æ³¨å†ŒåŒ¿åè´¦å·æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return None

    async def maintain_credentials(self):
        async with self.lock:
            while len(self.credentials) < settings.MIN_CREDENTIALS:
                if len(self.credentials) >= settings.MAX_CREDENTIALS:
                    logger.info(f"å‡­è¯æ± å·²æ»¡ ({len(self.credentials)}ä¸ª)ï¼Œåœæ­¢è¡¥å……ã€‚")
                    break
              
                new_token = await self._create_new_credential()
                if new_token:
                    self.credentials.append(new_token)
                else:
                    logger.warning("æ— æ³•åˆ›å»ºæ–°å‡­è¯ï¼Œå°†åœ¨ä¸‹æ¬¡ç»´æŠ¤æ—¶é‡è¯•ã€‚")
                    break

    async def get_credential(self) -> str:
        async with self.lock:
            if not self.credentials:
                logger.warning("å‡­è¯æ± ä¸ºç©ºï¼Œæ­£åœ¨ç´§æ€¥è¡¥å……...")
                await self.maintain_credentials()
                if not self.credentials:
                    raise Exception("æ— æ³•è·å–ä»»ä½•æœ‰æ•ˆå‡­è¯ã€‚")
          
            token = self.credentials.popleft()
            asyncio.create_task(self.maintain_credentials())
            logger.info(f"æä¾›ä¸€ä¸ªå‡­è¯ï¼Œæ± ä¸­å‰©ä½™: {len(self.credentials)}")
            return token

    async def close(self):
        await self.client.aclose()


--- æ–‡ä»¶è·¯å¾„: app\services\socketio_manager.py ---

import asyncio
import socketio
import json
from loguru import logger

class SocketIOManager:
    def __init__(self, access_token: str, socket_url: str):
        self.sio = socketio.AsyncClient(logger=False, engineio_logger=False)
        self.access_token = access_token
        self.socket_url = socket_url
        self.queue = asyncio.Queue()
        self.is_connected = asyncio.Event()
        self.is_finished = asyncio.Event()

        self._setup_event_handlers()

    def _setup_event_handlers(self):
        @self.sio.event
        async def connect():
            logger.success("Socket.IO è¿æ¥æˆåŠŸã€‚")
            self.is_connected.set()

        @self.sio.event
        async def disconnect():
            logger.warning("Socket.IO è¿æ¥æ–­å¼€ã€‚")
            self.is_connected.clear()
            if not self.is_finished.is_set():
                await self.queue.put(None)
                self.is_finished.set()

        # --- [ä¿®æ”¹] ç›‘å¬æ­£ç¡®çš„äº‹ä»¶åç§° 'execution:chunk' ---
        @self.sio.on('execution:chunk')
        async def on_execution_chunk(data):
            logger.debug(f"æ”¶åˆ° 'execution:chunk' äº‹ä»¶ï¼Œå†…å®¹: {data}")
            processed_data = data
            if isinstance(data, str):
                try:
                    processed_data = json.loads(data)
                except json.JSONDecodeError:
                    logger.warning(f"æ”¶åˆ°çš„å­—ç¬¦ä¸²ä¸æ˜¯æœ‰æ•ˆçš„JSONï¼Œå°†ä½œä¸ºæ™®é€šæ–‡æœ¬å¤„ç†: {data}")
                    processed_data = {"content": data}
          
            if not isinstance(processed_data, dict):
                 logger.warning(f"å¤„ç†åçš„æ•°æ®ä»ç„¶ä¸æ˜¯å­—å…¸ã€‚ç±»å‹: {type(processed_data)}")
                 processed_data = {"content": str(processed_data)}

            await self.queue.put(processed_data)
            
            # --- [æ–°å¢] æ£€æŸ¥å®ŒæˆçŠ¶æ€ ---
            if processed_data.get('status') == 'completed':
                logger.info("Socket.IO æ”¶åˆ° 'completed' çŠ¶æ€ï¼Œæ ‡è®°ä¸ºå®Œæˆã€‚")
                if not self.is_finished.is_set():
                    await self.queue.put(None)
                    self.is_finished.set()

        @self.sio.on('completion_finished')
        async def on_completion_finished(data):
            logger.info(f"Socket.IO æ”¶åˆ° 'completion_finished' äº‹ä»¶ã€‚æ•°æ®: {data}")
            if not self.is_finished.is_set():
                await self.queue.put(None)
                self.is_finished.set()
      
        @self.sio.on('*')
        async def catch_all(event, data):
            # --- [ä¿®æ”¹] è°ƒæ•´æœªçŸ¥äº‹ä»¶çš„æ—¥å¿—ï¼Œé¿å… 'execution:chunk' è¢«é‡å¤è®°å½• ---
            if event not in ['execution:chunk', 'completion_finished']:
                logger.info(f"æ•è·åˆ°æœªçŸ¥ Socket.IO äº‹ä»¶: '{event}', æ•°æ®: {data}")

    async def connect(self):
        try:
            await self.sio.connect(
                self.socket_url,
                auth={'token': self.access_token},
                transports=['websocket'],
                wait_timeout=20
            )
            await self.is_connected.wait()
        except Exception as e:
            logger.error(f"Socket.IO è¿æ¥å¤±è´¥: {e}", exc_info=True)
            self.is_finished.set()
            raise

    async def disconnect(self):
        if self.sio.connected:
            await self.sio.disconnect()

    async def get_data(self):
        return await self.queue.get()


--- æ–‡ä»¶è·¯å¾„: app\utils\sse_utils.py ---

import json
import time
from typing import Dict, Any, Optional

def create_sse_data(data: Dict[str, Any]) -> str:
    """å°†å­—å…¸æ•°æ®æ ¼å¼åŒ–ä¸º SSE äº‹ä»¶å­—ç¬¦ä¸²ã€‚"""
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

def create_chat_completion_chunk(
    request_id: str,
    model: str,
    content: str,
    finish_reason: Optional[str] = None
) -> Dict[str, Any]:
    """åˆ›å»ºä¸€ä¸ªä¸ OpenAI å…¼å®¹çš„èŠå¤©è¡¥å…¨æµå¼å—ã€‚"""
    return {
        "id": request_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "delta": {"content": content},
                "finish_reason": finish_reason
            }
        ]
    }

# --- [æ–°å¢] åˆ›å»ºä¸ OpenAI å…¼å®¹çš„éæµå¼èŠå¤©è¡¥å…¨å“åº” ---
def create_chat_completion_response(
    request_id: str,
    model: str,
    content: str,
    finish_reason: str
) -> Dict[str, Any]:
    """åˆ›å»ºä¸€ä¸ªä¸ OpenAI å…¼å®¹çš„å®Œæ•´ã€éæµå¼èŠå¤©è¡¥å…¨å“åº”ã€‚"""
    return {
        "id": request_id,
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content
                },
                "finish_reason": finish_reason
            }
        ],
        "usage": {
            "prompt_tokens": 0, # æ³¨æ„ï¼šç›®å‰æ— æ³•ä»ä¸Šæ¸¸è·å–å‡†ç¡®çš„ token æ•°
            "completion_tokens": 0,
            "total_tokens": 0
        }
    }

DONE_CHUNK = create_sse_data({"id": "done", "object": "done", "choices": [], "created": int(time.time()), "model": "done"})



